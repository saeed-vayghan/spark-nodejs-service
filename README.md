# Spark and Livy Dockerized Project + Node.js Service

This project showcases how to set up and run a Spark and Livy environment using Docker, along with running a Python script to process Parquet files using PySpark.
Additionally, it includes a Node.js application to interact with Livy and write data using `parquetjs-lite`.

## Prerequisites

- Docker
- Docker Compose
- Node.js

---

## Setup and Running the Project

### Step 1: Set Up Dockerfile for Livy and Spark Service

- `Dockerfile.livy` to configure the Livy service.
- `Dockerfile.spark` to configure the spark service.

### Step 2: Configure Docker Compose

- `docker-compose.yml` file to orchestrate the Spark Master, Spark Workers, and Livy services.

### Step 3: Setup Configuration Files

Ensure you have the following files:
- `spark.env` to set up spark environment variables.
- `livy.conf`  to set up livy environment variables.
- `log4j.properties` to configure logging. (optional)
- `spark-defaults.conf` to set Spark default configurations.
- `start-spark.sh` to start tbe spark cluster.
- `write_parquet.js` to define the Node.js writer script.
- `users.parquet` as the sample Parquet data file. this file gets generated by running write_parquet.js.
- `index.js` to define the Node.js API.
- `parquet_sql.py` to define the Spark job script.
- `fetch_logs.sh` to fetch job logs. (optional)
- `check_status.sh` to fetch job status. (optional)

### Step 4: Building and Running Docker Containers

1. **Build and Start the Docker Containers**:

    ```bash
    docker compose down
    docker compose build
    docker compose up -d
    ```

2. **Verify Docker Containers**:

- Ensure that Docker containers are running correctly:
- Check the livy ui on http://localhost:8998/ui

    ```bash
    $ docker ps

    # You should see output similar to this:
    # CONTAINER ID   IMAGE                               COMMAND                  CREATED        STATUS          PORTS                                              NAMES
    # <container_id> bde2020/spark-master:3.1.1-hadoop3.2 "/bin/sh -c '/bin/bash…" <time>         Up <time>       0.0.0.0:8080->8080/tcp, 0.0.0.0:7077->7077/tcp      spark-master
    # <container_id> bde2020/spark-worker:3.1.1-hadoop3.2 "/bin/sh -c '/bin/bash…" <time>         Up <time>       0.0.0.0:8081->8081/tcp                              spark-worker-1
    # <container_id> bde2020/spark-worker:3.1.1-hadoop3.2 "/bin/sh -c '/bin/bash…" <time>         Up <time>       0.0.0.0:8082->8082/tcp                              spark-worker-2
    # <container_id> your-livy-image                      "/bin/sh -c '/bin/bash…" <time>         Up <time>       0.0.0.0:8998->8998/tcp                              livy

    $ docker logs spark-master
    # You should see logs indicating that the Spark master has started and is registering worker nodes. Look for messages like:
    # INFO Master: I have been elected leader! New state: ALIVE
    # INFO Master: Registering worker <worker-id> with <number of cores> cores, <amount of memory> MB RAM

    $ docker logs spark-worker-1
    $ docker logs spark-worker-2
    # INFO Worker: Successfully registered with master spark://<master-url>
    ```


### Step 5: Write Data Using Node.js Writer Script

Ensure you have installed the necessary Node.js dependencies:

#### Note: 
- It is now up to you to elaborate the project to add more complex tasks.
- This repo contains a simple node.js app to write parquet file, you can implement a more complex micro-service to listen to different kafka messages, process data and then persist them as parquet files.
- This repo also contains a simple node API server to manage Spark jobs and a simple python script to run one specific Spark job, you can continue to work to have more complex Spark jobs. 

```bash
npm install

# to generate the users.parquet
node write_parquet.js

# start the API server
node index.js

# to run a spark job through node.js and live
curl -X POST http://localhost:3000/run-job

# to list all spark jobs
curl -X GET http://localhost:3000/jobs

# to get logs of a spark job
curl -X GET http://localhost:3000/jobs/:jobId/logs
```